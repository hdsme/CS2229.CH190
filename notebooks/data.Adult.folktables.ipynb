{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19db04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from folktables import ACSDataSource, ACSIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3656f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_dir = Path(\"/home/andre.cruz/Documents/fair-boosting/\")\n",
    "root_dir = Path(\"/mnt/home/andre.cruz/fair-boosting/\")\n",
    "data_dir = root_dir / \"data\" / \"Adult-2021\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# download 2018 data for 50 states\n",
    "state_list = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI',\n",
    "              'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI',\n",
    "              'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC',\n",
    "              'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT',\n",
    "              'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "data_source = ACSDataSource(\n",
    "    survey_year='2018', horizon='1-Year', survey='person',\n",
    "    root_dir=str(data_dir),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238affc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_COL = \"ST\"  # train/test splits will be stratified over states\n",
    "#SENSITIVE_COL = \"RAC1P\"  # default\n",
    "SENSITIVE_COL = \"SEX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1e36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is 3207990 rows x 286 columns\n",
    "data = data_source.get_data(states=state_list, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7799cb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1655429, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "dataset_details = deepcopy(ACSIncome)  # aka, 2021 Adult\n",
    "\n",
    "# Add State to the feature columns so we can do stratified splits (will be removed later)\n",
    "dataset_details.features.append(STATE_COL)\n",
    "\n",
    "# Pre-process + select Adult dataset features\n",
    "features, labels, groups = dataset_details.df_to_numpy(data)\n",
    "df = pd.DataFrame(data=features, columns=dataset_details.features)\n",
    "\n",
    "features.shape  # (1655429, 10) +1 for the state column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14fa3b4",
   "metadata": {},
   "source": [
    "## Order columns correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fcc5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset lists sensitive column as 'RAC1P' but we're using 'SEX'\n"
     ]
    }
   ],
   "source": [
    "df[dataset_details.target] = labels\n",
    "\n",
    "if SENSITIVE_COL != dataset_details.group:\n",
    "    print(f\"Dataset lists sensitive column as '{dataset_details.group}' \"\n",
    "          f\"but we're using '{SENSITIVE_COL}'\", file=sys.stderr)\n",
    "\n",
    "# Correct column ordering\n",
    "cols_order = [dataset_details.target, SENSITIVE_COL] + \\\n",
    "    list(set(dataset_details.features) - {SENSITIVE_COL, STATE_COL})\n",
    "\n",
    "state_col = df[STATE_COL]\n",
    "df = df[cols_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f36b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"COW\", # class of worker\n",
    "    #\"SCHL\", # educational attainment (categorical? there is a clear sequence between values...)\n",
    "    \"MAR\", # marital status\n",
    "    \"OCCP\", # occupation\n",
    "    \"POBP\", # place of birth\n",
    "    \"RELP\", # relationship\n",
    "    \"RAC1P\",\n",
    "]\n",
    "\n",
    "# Maintain column ordering\n",
    "ordered_cat_columns = [col for col in df.columns if col in set(categorical_columns)]\n",
    "\n",
    "types_dict = {col: int for col in categorical_columns}\n",
    "types_dict.update({\n",
    "    dataset_details.target: int,\n",
    "    \"SEX\": int,\n",
    "})\n",
    "\n",
    "# Set categorical columns to integers\n",
    "df = df.astype(types_dict)\n",
    "\n",
    "# Set sensitive column starting at value=0\n",
    "df[SENSITIVE_COL] = df[SENSITIVE_COL] - df[SENSITIVE_COL].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92c776",
   "metadata": {},
   "source": [
    "## Split Train/Test (Validation?)\n",
    "* According to the Datasheet, train/test splits are performed **stratified per state**:\n",
    "  * states are split **80%/20%**, and resulting data is aggregated in US-wide train/test datasets;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca29340",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TEST_PCT = 20\n",
    "TRAIN_PCT = 60\n",
    "\n",
    "VAL_PCT = 100 - TRAIN_PCT - TEST_PCT\n",
    "assert VAL_PCT >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3943e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, other_idx = train_test_split(\n",
    "    df.index,\n",
    "    train_size=0.01 * TRAIN_PCT,\n",
    "    stratify=state_col,\n",
    "    random_state=SEED, shuffle=True)\n",
    "\n",
    "train_df, other_df = df.loc[train_idx], df.loc[other_idx]\n",
    "assert len(set(train_idx) & set(other_idx)) == 0\n",
    "\n",
    "# Split validation\n",
    "if VAL_PCT > 0:\n",
    "    new_test_pct = TEST_PCT / (TEST_PCT + VAL_PCT)\n",
    "\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        other_df.index,\n",
    "        test_size=new_test_pct,\n",
    "        stratify=state_col.loc[other_idx],\n",
    "        random_state=SEED, shuffle=True)\n",
    "\n",
    "    val_df, test_df = other_df.loc[val_idx], other_df.loc[test_idx]\n",
    "    assert len(train_idx) + len(val_idx) + len(test_idx) == len(df)\n",
    "\n",
    "assert np.isclose(len(train_df) / len(df), 0.01 * TRAIN_PCT)\n",
    "assert np.isclose(len(test_df) / len(df), 0.01 * TEST_PCT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209bd3a",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# One-hot encoding section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90142bee",
   "metadata": {},
   "source": [
    "### 1. Join all values that represent less than 1% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f5845c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_number_of_categories(df, rel_threshold=0.01, fill_value=-1, set_start_at_zero=False):\n",
    "    out_df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in categorical_columns:\n",
    "            if out_df[col].nunique() <= 2: continue\n",
    "            # Set value count starting at zero\n",
    "            if set_start_at_zero:\n",
    "                out_df[col] = out_df[col] - out_df[col].min()\n",
    "\n",
    "            # Reduce the number of categories to only those that appear more than (rel_threshold * 100)%\n",
    "            counts = out_df[col].value_counts(normalize=True)\n",
    "            cats_above_threshold = set(counts[counts > rel_threshold].index)\n",
    "\n",
    "            out_df[col] = out_df[col].map(lambda v: v if v in cats_above_threshold else fill_value)\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2aefc",
   "metadata": {},
   "source": [
    "### 1.1. Or just one-hot encode *everything*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d0c520",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MERGE_LOW_PREVALENCE_VALUES = False\n",
    "\n",
    "# Join feature values with low dataset prevalence\n",
    "if MERGE_LOW_PREVALENCE_VALUES:\n",
    "    df_preprocessed = reduce_number_of_categories(df, rel_threshold=0.01, set_start_at_zero=True)\n",
    "\n",
    "# Or simply let everything as is :)\n",
    "else:\n",
    "    df_preprocessed = reduce_number_of_categories(df, rel_threshold=0.0001, set_start_at_zero=True)\n",
    "    #df_preprocessed = df.copy()  # we should use this one really, but takes too much RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e8883",
   "metadata": {},
   "source": [
    "### 2. One-hot encode remaining values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca6d9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5f5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categoricals_onehot = enc.fit_transform(df_preprocessed[ordered_cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85645235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "df_onehot = OrderedDict()\n",
    "\n",
    "col_count = 0\n",
    "for col, values in zip(ordered_cat_columns, enc.categories_):\n",
    "    for v_idx, v in enumerate(values):\n",
    "        df_onehot[f\"{col}_{v}\"] = df_categoricals_onehot[:, col_count + v_idx].astype(int)\n",
    "    \n",
    "    col_count += len(values)\n",
    "\n",
    "# Add label and remaining numerical features\n",
    "df_onehot.update({\n",
    "    col: df[col].to_numpy() for col in df.columns if col not in set(ordered_cat_columns)\n",
    "})\n",
    "\n",
    "df_onehot.move_to_end(SENSITIVE_COL, last=False)\n",
    "df_onehot.move_to_end(dataset_details.target, last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c864b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PINCP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>OCCP_-1</th>\n",
       "      <th>OCCP_0</th>\n",
       "      <th>OCCP_10</th>\n",
       "      <th>OCCP_30</th>\n",
       "      <th>OCCP_41</th>\n",
       "      <th>OCCP_42</th>\n",
       "      <th>OCCP_50</th>\n",
       "      <th>OCCP_91</th>\n",
       "      <th>...</th>\n",
       "      <th>POBP_460</th>\n",
       "      <th>POBP_461</th>\n",
       "      <th>POBP_466</th>\n",
       "      <th>POBP_468</th>\n",
       "      <th>POBP_500</th>\n",
       "      <th>POBP_507</th>\n",
       "      <th>POBP_514</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>WKHP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655424</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655425</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655426</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655427</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655428</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1655429 rows × 716 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PINCP  SEX  OCCP_-1  OCCP_0  OCCP_10  OCCP_30  OCCP_41  OCCP_42  \\\n",
       "0            0    1        0       0        0        0        0        0   \n",
       "1            0    0        0       0        0        0        0        0   \n",
       "2            0    0        0       0        0        0        0        0   \n",
       "3            0    1        0       0        0        0        0        0   \n",
       "4            0    0        0       0        0        0        0        0   \n",
       "...        ...  ...      ...     ...      ...      ...      ...      ...   \n",
       "1655424      0    1        0       0        0        0        0        0   \n",
       "1655425      0    0        0       0        0        0        0        0   \n",
       "1655426      0    1        0       0        0        0        0        0   \n",
       "1655427      0    1        0       0        0        0        0        0   \n",
       "1655428      0    1        0       0        0        0        0        0   \n",
       "\n",
       "         OCCP_50  OCCP_91  ...  POBP_460  POBP_461  POBP_466  POBP_468  \\\n",
       "0              0        0  ...         0         0         0         0   \n",
       "1              0        0  ...         0         0         0         0   \n",
       "2              0        0  ...         0         0         0         0   \n",
       "3              0        0  ...         0         0         0         0   \n",
       "4              0        0  ...         0         0         0         0   \n",
       "...          ...      ...  ...       ...       ...       ...       ...   \n",
       "1655424        0        0  ...         0         0         0         0   \n",
       "1655425        0        0  ...         0         0         0         0   \n",
       "1655426        0        0  ...         0         0         0         0   \n",
       "1655427        0        0  ...         0         0         0         0   \n",
       "1655428        0        0  ...         0         0         0         0   \n",
       "\n",
       "         POBP_500  POBP_507  POBP_514  SCHL  AGEP  WKHP  \n",
       "0               0         0         0  18.0  18.0  21.0  \n",
       "1               0         0         0  17.0  53.0  40.0  \n",
       "2               0         0         0  16.0  41.0  40.0  \n",
       "3               0         0         0  18.0  18.0   2.0  \n",
       "4               0         0         0  19.0  21.0  50.0  \n",
       "...           ...       ...       ...   ...   ...   ...  \n",
       "1655424         0         0         0  20.0  55.0  30.0  \n",
       "1655425         0         0         0  18.0  41.0  40.0  \n",
       "1655426         0         0         0  21.0  34.0  50.0  \n",
       "1655427         0         0         0  19.0  49.0  40.0  \n",
       "1655428         0         0         0  16.0  19.0  40.0  \n",
       "\n",
       "[1655429 rows x 716 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot = pd.DataFrame(df_onehot)\n",
    "df_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1aed8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_onehot = df_onehot.loc[train_idx]\n",
    "train_df_onehot.to_csv(data_dir / f\"ACSIncome.train.onehot.header{'.all-values' if not MERGE_LOW_PREVALENCE_VALUES else ''}.csv\", sep='\\t', header=True)\n",
    "\n",
    "test_df_onehot = df_onehot.loc[test_idx]\n",
    "test_df_onehot.to_csv(data_dir / f\"ACSIncome.test.onehot.header{'.all-values' if not MERGE_LOW_PREVALENCE_VALUES else ''}.csv\", sep='\\t', header=True)\n",
    "\n",
    "if VAL_PCT > 0:\n",
    "    val_df_onehot = df_onehot.loc[val_idx]\n",
    "    val_df_onehot.to_csv(data_dir / f\"ACSIncome.validation.onehot.header{'.all-values' if not MERGE_LOW_PREVALENCE_VALUES else ''}.csv\", sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc32cb",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a2d52",
   "metadata": {},
   "source": [
    "## Write DFs to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22f91362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist train and test DFs\n",
    "df.to_csv(data_dir / \"ACSIncome.csv\")\n",
    "\n",
    "train_df.to_csv(data_dir / \"ACSIncome.train.csv\")\n",
    "train_df.to_csv(data_dir / \"ACSIncome.train.header.csv\", sep='\\t', header=True, index_label=\"index\")\n",
    "train_df.to_csv(data_dir / \"ACSIncome.train.preprocessed-for-lightgbm-cpp.csv\", sep='\\t', header=False)\n",
    "\n",
    "test_df.to_csv(data_dir / \"ACSIncome.test.csv\")\n",
    "test_df.to_csv(data_dir / \"ACSIncome.test.header.csv\", sep='\\t', header=True, index_label=\"index\")\n",
    "test_df.to_csv(data_dir / \"ACSIncome.test.preprocessed-for-lightgbm-cpp.csv\", sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87fc804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAL_PCT > 0:\n",
    "    val_df.to_csv(data_dir / \"ACSIncome.validation.csv\")\n",
    "    val_df.to_csv(data_dir / \"ACSIncome.validation.header.csv\", sep='\\t', header=True, index_label=\"index\")\n",
    "    val_df.to_csv(data_dir / \"ACSIncome.validation.preprocessed-for-lightgbm-cpp.csv\", sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea0dbf",
   "metadata": {},
   "source": [
    "## Write version for LightGBM (C++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f79674d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write column ordering to disk\n",
    "cols_file_path = data_dir / \"cols_order.csv\"\n",
    "with open(cols_file_path, \"w\") as out_f:\n",
    "    out_f.write(\",\".join([\"index\"] + cols_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94ee4a",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# Equalize prevalences among protected groups (unserample LNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "342d0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1e-5\n",
    "LABEL_COL = dataset_details.target\n",
    "\n",
    "# Maximum prevalence\n",
    "max_prevalence = train_df.groupby(SENSITIVE_COL).mean()[LABEL_COL].max()\n",
    "\n",
    "train_df_eq_prev: pd.DataFrame = None\n",
    "for g in train_df[SENSITIVE_COL].unique():\n",
    "    group_data = train_df[train_df[SENSITIVE_COL] == g]\n",
    "    \n",
    "    if group_data[LABEL_COL].mean() < max_prevalence - delta:\n",
    "        group_data_LP = group_data[group_data[LABEL_COL] == 1]\n",
    "        group_data_LN = group_data[group_data[LABEL_COL] == 0]\n",
    "        \n",
    "        LPs = len(group_data_LP)\n",
    "        LNs = len(group_data_LN)\n",
    "        assert LPs + LNs == len(group_data)\n",
    "\n",
    "        # Compute number of LNs to achieve target prevalence\n",
    "        target_num_LN = int((LPs / max_prevalence) - LPs)\n",
    "\n",
    "        group_data = pd.concat(\n",
    "            (\n",
    "                group_data_LP,\n",
    "                group_data_LN.sample(n=target_num_LN),\n",
    "            ),\n",
    "            axis=0,\n",
    "        ).sample(frac=1) # re-shuffle rows\n",
    "\n",
    "    if train_df_eq_prev is None:\n",
    "        train_df_eq_prev = group_data\n",
    "    else:\n",
    "        train_df_eq_prev = pd.concat(\n",
    "            (train_df_eq_prev, group_data),\n",
    "            axis=0)\n",
    "\n",
    "assert all(train_df_eq_prev.groupby(SENSITIVE_COL).mean()[LABEL_COL] >= max_prevalence - delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59dcadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEX\n",
       "0    0.442827\n",
       "1    0.291470\n",
       "Name: PINCP, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(SENSITIVE_COL).mean()[LABEL_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "985ca5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEX\n",
       "0    0.442827\n",
       "1    0.442828\n",
       "Name: PINCP, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_eq_prev.groupby(SENSITIVE_COL).mean()[LABEL_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7b6bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_eq_prev.to_csv(data_dir / \"ACSIncome.train-equal-prev.csv\")\n",
    "train_df_eq_prev.to_csv(data_dir / \"ACSIncome.train-equal-prev.header.csv\", sep='\\t', header=True, index_label=\"index\")\n",
    "train_df_eq_prev.to_csv(data_dir / \"ACSIncome.train-equal-prev.preprocessed-for-lightgbm-cpp.csv\", sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a77b0",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anonymized (cluster)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
