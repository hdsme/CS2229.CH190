{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d7192f",
   "metadata": {},
   "source": [
    "# TODO!\n",
    "1. load data, set-up boilerplate\n",
    "2. load all hyperparameter spaces\n",
    "3. sample n samples from each\n",
    "4. launch train-evaluate functions for each sample\n",
    "5. join everything in a DF following the same structure as the GBM experiments\n",
    "6. save everything to disk\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1cace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET_NAME = \"AOF-FairHO\"\n",
    "#DATASET_NAME = \"AOF-Fairbench\"\n",
    "DATASET_NAME = \"Adult-2021\"\n",
    "\n",
    "#EXPERIMENT_NAME = \"baselines\"\n",
    "EXPERIMENT_NAME = \"baselines/Fairlearn-EG\"\n",
    "\n",
    "# Number of Random Search trials per algorithm\n",
    "N_RS_TRIALS = 100\n",
    "\n",
    "# Set test threshold on validation ?\n",
    "SET_TEST_THRESHOLD_ON_VALIDATION = False\n",
    "\n",
    "N_THREADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 24\n",
    "\n",
    "from random import Random\n",
    "rng = Random(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b65ac",
   "metadata": {},
   "source": [
    "# Relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import get_dataset_details\n",
    "dataset_details = get_dataset_details(DATASET_NAME, local_or_cluster=\"local\")\n",
    "\n",
    "# Expose dataset details as global variables :)\n",
    "globals().update(dataset_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353eb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = {\n",
    "    \"train\": train_data_path,\n",
    "    \"validation\": val_data_path,\n",
    "    \"test\": test_data_path,\n",
    "}\n",
    "\n",
    "experiment_path = root_path / \"experiments\" / DATASET_NAME\n",
    "configs_path = experiment_path / \"confs\" / EXPERIMENT_NAME\n",
    "results_path = experiment_path / \"results\" / EXPERIMENT_NAME\n",
    "results_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea7f79",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfa0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_s(data, label_col, sensitive_col, unawareness: bool = False):\n",
    "    ignored_cols = {label_col, sensitive_col} if unawareness else {label_col}\n",
    "    feature_cols = [col for col in data.columns if col not in ignored_cols]\n",
    "    return (\n",
    "        data[feature_cols],\n",
    "        data[label_col].to_numpy(dtype=int),\n",
    "        data[sensitive_col].to_numpy(dtype=int))\n",
    "\n",
    "## Train data is not needed (and some models may even use different train datasets)\n",
    "data = {}\n",
    "for key, val in data_path.items():\n",
    "    if val is None: continue\n",
    "\n",
    "    df = pd.read_csv(val, sep=\"\\t\", index_col=0, header=0)\n",
    "    X, y, s = get_X_y_s(\n",
    "        df,\n",
    "        label_col=label_col,\n",
    "        sensitive_col=sensitive_col,\n",
    "    )\n",
    "    \n",
    "    data[key] = df\n",
    "    data[f\"X_{key}\"] = X\n",
    "    data[f\"y_{key}\"] = y\n",
    "    data[f\"s_{key}\"] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5f641",
   "metadata": {},
   "source": [
    "# Load hyperparameter spaces\n",
    "- load all hyperparameter YAML files under the \\<experiment\\>/confs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfbf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utils.fairautoml_tuners_utils import load_hyperparameter_space\n",
    "\n",
    "yaml_regex = re.compile(r\"^(?P<name>.+)[\\.]hyperparameter-space.yaml$\")\n",
    "hyperparam_spaces = dict()\n",
    "\n",
    "for file_name in os.listdir(configs_path):\n",
    "    m = yaml_regex.match(file_name)\n",
    "\n",
    "    if m:\n",
    "        file_path = configs_path / file_name\n",
    "        hyperparam_spaces[m.group(\"name\")] = load_hyperparameter_space(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f153bc3",
   "metadata": {},
   "source": [
    "# Randomly sample _n_ hyperparameter configs per algorithm\n",
    "- and write to files under the confs/\\<algorithm\\> folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463940bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hyperparams import suggest_random_hyperparams_with_classpath\n",
    "\n",
    "configs = {\n",
    "    algo_name: [\n",
    "        suggest_random_hyperparams_with_classpath(hyper_space, seed=rng.randrange(2**32 - 1))\n",
    "        for _ in range(N_RS_TRIALS)\n",
    "    ]\n",
    "    for algo_name, hyper_space in hyperparam_spaces.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe07091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configs to disk\n",
    "import json\n",
    "\n",
    "configs_file_path = configs_path / f\"configs.{N_RS_TRIALS}-trials-per-algo.json\"\n",
    "with open(configs_file_path, \"w\") as out_file:\n",
    "    json.dump(configs, out_file, indent=4)\n",
    "    print(f\"Saved configs to JSON file at '{configs_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795d314",
   "metadata": {},
   "source": [
    "# Train models, evaluate, and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# ** Set-up run_trial function with common kwargs\n",
    "###\n",
    "from utils.model_evaluation import try_hyperparams\n",
    "from tqdm import tqdm\n",
    "\n",
    "progress_bar = tqdm(\n",
    "    total=sum(len(algo_configs) for algo_configs in configs.values()),\n",
    "    desc=\"Progress\", position=0)\n",
    "\n",
    "def run_trial(hyperparams):\n",
    "    ret = dict()\n",
    "\n",
    "    try:\n",
    "        ret = try_hyperparams(\n",
    "            hyperparams,\n",
    "            data=data,\n",
    "            eval_on_train=False,\n",
    "            target_metric=target_metric,\n",
    "            target_value=target_value,\n",
    "            set_test_threshold_on_validation=SET_TEST_THRESHOLD_ON_VALIDATION,\n",
    "            n_threads=1,  # this is actually ignored for all models except LGBM\n",
    "        )\n",
    "    except RuntimeError as err:\n",
    "        logging.error(f\"Trial failed with error '{err}'\")\n",
    "    finally:\n",
    "        progress_bar.update()\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "###\n",
    "# ** Create a ThreadPool and launch trials **\n",
    "###\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "experiment_results = dict()\n",
    "\n",
    "for algo_name, algo_configs in configs.items():\n",
    "    with ThreadPoolExecutor(max_workers=N_THREADS) as thread_pool:\n",
    "        algo_results = thread_pool.map(run_trial, algo_configs)\n",
    "\n",
    "    # Expand results iterator to a list (all threads have finished)\n",
    "    experiment_results[algo_name] = list(algo_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef2835",
   "metadata": {},
   "source": [
    "# Save results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72eae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_NAME = (\n",
    "    f\"{DATASET_NAME}.BASELINE-results.\"\n",
    "    f\"{N_RS_TRIALS}-trials-per-algo.\"\n",
    "    f\"{target_metric}={target_value:.2}.\"\n",
    "    f\"use-val-threshold={SET_TEST_THRESHOLD_ON_VALIDATION}\"\n",
    "    #f\".randomized-clf={FAIRGBM_RANDOMIZED_CLASSIFIER}\"\n",
    ")\n",
    "RESULTS_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f7557",
   "metadata": {},
   "source": [
    "### Save dict as a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import NumpyJSONEncoder\n",
    "\n",
    "# Save JSON file\n",
    "results_file_path = results_path / f\"{RESULTS_NAME}.json\"\n",
    "with open(results_file_path, \"w\") as out_file:\n",
    "    json.dump(experiment_results, out_file, indent=4, cls=NumpyJSONEncoder)\n",
    "    print(f\"Saved JSON results to file at '{results_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130e1df",
   "metadata": {},
   "source": [
    "### Save organized results as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'time-taken' to form DF\n",
    "tmp_results_for_df = {\n",
    "    frst_lvl: [\n",
    "        {\n",
    "            inner_key: inner_val\n",
    "            for inner_key, inner_val in curr_eval.items()\n",
    "            if inner_key != \"time-taken\"\n",
    "        }\n",
    "        for curr_eval in experiment_results[frst_lvl]\n",
    "    ]\n",
    "    for frst_lvl in experiment_results\n",
    "}\n",
    "\n",
    "# Flatten results in order to convert them to a DataFrame\n",
    "results_df = {\n",
    "    (f\"{frst_lvl}.{scnd_lvl:03}\", dataset_type): dataset_result\n",
    "    for frst_lvl, frst_lvl_list in tmp_results_for_df.items()\n",
    "    for scnd_lvl, scnd_lvl_dict in enumerate(frst_lvl_list)\n",
    "    for dataset_type, dataset_result in scnd_lvl_dict.items()\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_df).transpose()\n",
    "results_df\n",
    "\n",
    "# Save to disk\n",
    "df_file_path = results_path / f\"{RESULTS_NAME}.csv\"\n",
    "results_df.to_csv(df_file_path)\n",
    "print(f\"Saved CSV results to file at '{df_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f42eb7",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# Some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(df_file_path, index_col=(0,1))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdce4d",
   "metadata": {},
   "source": [
    "## Test results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9852689",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only TEST results\n",
    "test_results_table = results_df.loc[pd.IndexSlice[:, \"test\"], :].droplevel(1, axis=0)\n",
    "test_results_table[perf_metric] = 1. - test_results_table[\"fnr\"]\n",
    "test_results_table = test_results_table.drop(columns=\"fnr\")\n",
    "\n",
    "eval_columns = [\n",
    "    perf_metric,\n",
    "    fair_metric,\n",
    "    \"fpr_diff\",\n",
    "    \"fnr_diff\",\n",
    "    \"roc_auc\",\n",
    "    \"threshold\",\n",
    "]\n",
    "test_results_table = test_results_table[eval_columns]\n",
    "test_results_table[\"model\"] = [model_idx[:-4] for model_idx in test_results_table.index]\n",
    "test_results_table[\"id\"] = [int(model_idx[-3:]) for model_idx in test_results_table.index]\n",
    "\n",
    "test_results_table.sort_values(by=perf_metric, axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd519b",
   "metadata": {},
   "source": [
    "## Validation results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8525c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only VALIDATION results\n",
    "val_results_table = results_df.loc[pd.IndexSlice[:, \"validation\"], :].droplevel(1, axis=0)\n",
    "val_results_table[perf_metric] = 1. - val_results_table[\"fnr\"]\n",
    "val_results_table = val_results_table.drop(columns=\"fnr\")\n",
    "\n",
    "eval_columns = [\n",
    "    perf_metric,\n",
    "    fair_metric,\n",
    "    \"fpr_diff\",\n",
    "    \"fnr_diff\",\n",
    "    \"roc_auc\",\n",
    "    \"threshold\",\n",
    "]\n",
    "val_results_table = val_results_table[eval_columns]\n",
    "val_results_table[\"model\"] = [model_idx[:-4] for model_idx in val_results_table.index]\n",
    "val_results_table[\"id\"] = [int(model_idx[-3:]) for model_idx in val_results_table.index]\n",
    "\n",
    "val_results_table.sort_values(by=perf_metric, axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832a7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
