# **************
# ** TEMPLATE **
# **************
#
# Common configurations between all FairGBM / LightGBM models.
#
verbosity = 0   # only warnings or errors

task = train
# eval metrics, support multi metric, delimited by ',' , support following metrics
# metric = binary_logloss,auc
# metric_freq = 5
# true if need output metric for training data, alias: tranining_metric, train_metric
# is_training_metric = true
# number of bins for feature bucket, 255 is a recommend setting, it can save memories, and also has good accuracy.
max_bin = 255
# training data
# if existing weight file, should name to "binary.train.weight"
# alias: train_data, train
# data = /home/andre.cruz/Documents/fair-boosting/data/AOF-FairHO/pre-processed_train.header.csv
has_header = True
label_column = name:fraud_bool
#ignore_column = name:index          # columns to ignore (index)
# categorical_feature = # does not count label, column values should be consecutive ints

# validation data, support multi validation data, separated by ','
# if existing weight file, should name to "binary.test.weight"
# alias: valid, test, test_data,
valid_data = /mnt/home/andre.cruz/fair-boosting/data/AOF-FairHO/pre-processed_validation.header.csv

num_threads = 1
random_state = 3407

# **NOTE** remaining hyperparameters will be randomly sampled and added to this file
data = /mnt/home/andre.cruz/fair-boosting/data/AOF-FairHO/pre-processed_train.header.csv
ignore_column = name:index
boosting_type = goss
enable_bundle = False
n_estimators = 3510
num_leaves = 71
min_child_samples = 39
max_depth = 13
learning_rate = 0.13282438961243906
reg_alpha = 0.04686825053508049
reg_lambda = 0.08464569478116121
output_dir = /mnt/home/andre.cruz/fair-boosting/experiments/AOF-FairHO/results/randomly-generated-configs/LightGBM/059
output_model = /mnt/home/andre.cruz/fair-boosting/experiments/AOF-FairHO/results/randomly-generated-configs/LightGBM/059/model.txt
